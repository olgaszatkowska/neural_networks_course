{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training experimental models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset.dataset import load_fashion_mnist_dataset\n",
    "\n",
    "X_train, X_test = load_fashion_mnist_dataset()\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_dim = X_train[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from models.metrics import MeanSquaredError\n",
    "from models.neural_network import Autoencoder\n",
    "from models.optimizer import Optimizer\n",
    "\n",
    "\n",
    "def pickle_results(\n",
    "    dim: int,\n",
    "    learning_rate: float,\n",
    "    batch_size: int,\n",
    "    optimizer: Optimizer,\n",
    "    autoencoder: Autoencoder,\n",
    "):\n",
    "    with open(\n",
    "        f\"saved_models/networks/autoencoder_{dim}_{learning_rate}_{batch_size}\", \"wb\"\n",
    "    ) as pickle_file:\n",
    "        pickle.dump(autoencoder, pickle_file)\n",
    "\n",
    "    with open(\n",
    "        f\"saved_models/optimizers/optimizer_{dim}_{learning_rate}_{batch_size}\", \"wb\"\n",
    "    ) as pickle_file:\n",
    "        pickle.dump(optimizer, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dims = [12, 36, 108]\n",
    "learning_rates = [1, 0.9, 0.09]\n",
    "batch_sizes = [16, 128, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code dim\n",
    "\n",
    "learning_rate = learning_rates[1]\n",
    "batch_size = batch_sizes[1]\n",
    "\n",
    "for code_dim in code_dims:\n",
    "    autoencoder = Autoencoder(\n",
    "        input_dim=flatten_dim, code_dim=code_dim, encoder_hidden_count=0, reduce_by=1.5\n",
    "    )\n",
    "    optimizer = Optimizer(\n",
    "        autoencoder,\n",
    "        loss=MeanSquaredError(),\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=150,\n",
    "    )\n",
    "\n",
    "    optimizer.fit(X_train, X_train)\n",
    "\n",
    "    pickle_results(code_dim, learning_rate, batch_size, optimizer, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  -- accuracy 0.001 - loss 0.133\n",
      "Epoch 1  -- accuracy 0.001 - loss 0.091\n",
      "Epoch 2  -- accuracy 0.001 - loss 0.087\n",
      "Epoch 3  -- accuracy 0.001 - loss 0.085\n",
      "Epoch 4  -- accuracy 0.001 - loss 0.082\n",
      "Epoch 5  -- accuracy 0.002 - loss 0.076\n",
      "Epoch 6  -- accuracy 0.003 - loss 0.069\n",
      "Epoch 7  -- accuracy 0.004 - loss 0.063\n",
      "Epoch 8  -- accuracy 0.004 - loss 0.058\n",
      "Epoch 9  -- accuracy 0.004 - loss 0.053\n",
      "Epoch 10  -- accuracy 0.004 - loss 0.050\n",
      "Epoch 11  -- accuracy 0.004 - loss 0.047\n",
      "Epoch 12  -- accuracy 0.004 - loss 0.045\n",
      "Epoch 13  -- accuracy 0.004 - loss 0.044\n",
      "Epoch 14  -- accuracy 0.005 - loss 0.043\n",
      "Epoch 15  -- accuracy 0.006 - loss 0.041\n",
      "Epoch 16  -- accuracy 0.006 - loss 0.040\n",
      "Epoch 17  -- accuracy 0.007 - loss 0.040\n",
      "Epoch 18  -- accuracy 0.007 - loss 0.039\n",
      "Epoch 19  -- accuracy 0.007 - loss 0.038\n",
      "Epoch 20  -- accuracy 0.007 - loss 0.038\n",
      "Epoch 21  -- accuracy 0.007 - loss 0.037\n",
      "Epoch 22  -- accuracy 0.008 - loss 0.037\n",
      "Epoch 23  -- accuracy 0.008 - loss 0.037\n",
      "Epoch 24  -- accuracy 0.008 - loss 0.037\n",
      "Epoch 25  -- accuracy 0.008 - loss 0.036\n",
      "Epoch 26  -- accuracy 0.008 - loss 0.036\n",
      "Epoch 27  -- accuracy 0.008 - loss 0.036\n",
      "Epoch 28  -- accuracy 0.009 - loss 0.035\n",
      "Epoch 29  -- accuracy 0.008 - loss 0.035\n",
      "Epoch 30  -- accuracy 0.008 - loss 0.034\n",
      "Epoch 31  -- accuracy 0.008 - loss 0.034\n",
      "Epoch 32  -- accuracy 0.008 - loss 0.034\n",
      "Epoch 33  -- accuracy 0.008 - loss 0.033\n",
      "Epoch 34  -- accuracy 0.009 - loss 0.033\n",
      "Epoch 35  -- accuracy 0.009 - loss 0.033\n",
      "Epoch 36  -- accuracy 0.009 - loss 0.033\n",
      "Epoch 37  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 38  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 39  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 40  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 41  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 42  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 43  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 44  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 45  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 46  -- accuracy 0.012 - loss 0.030\n",
      "Epoch 47  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 48  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 49  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 50  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 51  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 52  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 53  -- accuracy 0.011 - loss 0.029\n",
      "Epoch 54  -- accuracy 0.011 - loss 0.029\n",
      "Epoch 55  -- accuracy 0.011 - loss 0.029\n",
      "Epoch 56  -- accuracy 0.011 - loss 0.029\n",
      "Epoch 57  -- accuracy 0.011 - loss 0.029\n",
      "Epoch 58  -- accuracy 0.011 - loss 0.029\n",
      "Epoch 59  -- accuracy 0.011 - loss 0.029\n",
      "Epoch 60  -- accuracy 0.012 - loss 0.028\n",
      "Epoch 61  -- accuracy 0.011 - loss 0.028\n",
      "Epoch 62  -- accuracy 0.011 - loss 0.028\n",
      "Epoch 63  -- accuracy 0.011 - loss 0.028\n",
      "Epoch 64  -- accuracy 0.011 - loss 0.028\n",
      "Epoch 65  -- accuracy 0.011 - loss 0.028\n",
      "Epoch 66  -- accuracy 0.011 - loss 0.028\n",
      "Epoch 67  -- accuracy 0.011 - loss 0.027\n",
      "Epoch 68  -- accuracy 0.011 - loss 0.027\n",
      "Epoch 69  -- accuracy 0.011 - loss 0.027\n",
      "Epoch 70  -- accuracy 0.011 - loss 0.027\n",
      "Epoch 71  -- accuracy 0.011 - loss 0.027\n",
      "Epoch 72  -- accuracy 0.011 - loss 0.027\n",
      "Epoch 73  -- accuracy 0.012 - loss 0.027\n",
      "Epoch 74  -- accuracy 0.012 - loss 0.027\n",
      "Epoch 75  -- accuracy 0.012 - loss 0.027\n",
      "Epoch 76  -- accuracy 0.012 - loss 0.027\n",
      "Epoch 77  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 78  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 79  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 80  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 81  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 82  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 83  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 84  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 85  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 86  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 87  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 88  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 89  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 90  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 91  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 92  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 93  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 94  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 95  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 96  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 97  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 98  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 99  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 100  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 101  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 102  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 103  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 104  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 105  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 106  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 107  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 108  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 109  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 110  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 111  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 112  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 113  -- accuracy 0.012 - loss 0.025\n",
      "Epoch 114  -- accuracy 0.012 - loss 0.025\n",
      "Epoch 115  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 116  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 117  -- accuracy 0.011 - loss 0.024\n",
      "Epoch 118  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 119  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 120  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 121  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 122  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 123  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 124  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 125  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 126  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 127  -- accuracy 0.012 - loss 0.024\n",
      "Epoch 128  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 129  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 130  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 131  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 132  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 133  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 134  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 135  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 136  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 137  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 138  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 139  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 140  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 141  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 142  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 143  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 144  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 145  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 146  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 147  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 148  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 149  -- accuracy 0.013 - loss 0.024\n",
      "Epoch 0  -- accuracy 0.001 - loss 0.193\n",
      "Epoch 1  -- accuracy 0.001 - loss 0.168\n",
      "Epoch 2  -- accuracy 0.001 - loss 0.145\n",
      "Epoch 3  -- accuracy 0.001 - loss 0.123\n",
      "Epoch 4  -- accuracy 0.001 - loss 0.104\n",
      "Epoch 5  -- accuracy 0.001 - loss 0.089\n",
      "Epoch 6  -- accuracy 0.001 - loss 0.079\n",
      "Epoch 7  -- accuracy 0.001 - loss 0.073\n",
      "Epoch 8  -- accuracy 0.001 - loss 0.070\n",
      "Epoch 9  -- accuracy 0.001 - loss 0.068\n",
      "Epoch 10  -- accuracy 0.001 - loss 0.067\n",
      "Epoch 11  -- accuracy 0.001 - loss 0.066\n",
      "Epoch 12  -- accuracy 0.001 - loss 0.066\n",
      "Epoch 13  -- accuracy 0.001 - loss 0.065\n",
      "Epoch 14  -- accuracy 0.001 - loss 0.065\n",
      "Epoch 15  -- accuracy 0.001 - loss 0.065\n",
      "Epoch 16  -- accuracy 0.001 - loss 0.065\n",
      "Epoch 17  -- accuracy 0.002 - loss 0.064\n",
      "Epoch 18  -- accuracy 0.002 - loss 0.064\n",
      "Epoch 19  -- accuracy 0.002 - loss 0.064\n",
      "Epoch 20  -- accuracy 0.002 - loss 0.064\n",
      "Epoch 21  -- accuracy 0.002 - loss 0.063\n",
      "Epoch 22  -- accuracy 0.002 - loss 0.063\n",
      "Epoch 23  -- accuracy 0.002 - loss 0.063\n",
      "Epoch 24  -- accuracy 0.002 - loss 0.063\n",
      "Epoch 25  -- accuracy 0.002 - loss 0.062\n",
      "Epoch 26  -- accuracy 0.002 - loss 0.062\n",
      "Epoch 27  -- accuracy 0.002 - loss 0.062\n",
      "Epoch 28  -- accuracy 0.002 - loss 0.062\n",
      "Epoch 29  -- accuracy 0.002 - loss 0.061\n",
      "Epoch 30  -- accuracy 0.002 - loss 0.061\n",
      "Epoch 31  -- accuracy 0.002 - loss 0.061\n",
      "Epoch 32  -- accuracy 0.002 - loss 0.060\n",
      "Epoch 33  -- accuracy 0.002 - loss 0.060\n",
      "Epoch 34  -- accuracy 0.002 - loss 0.060\n",
      "Epoch 35  -- accuracy 0.002 - loss 0.059\n",
      "Epoch 36  -- accuracy 0.002 - loss 0.059\n",
      "Epoch 37  -- accuracy 0.002 - loss 0.058\n",
      "Epoch 38  -- accuracy 0.002 - loss 0.058\n",
      "Epoch 39  -- accuracy 0.002 - loss 0.057\n",
      "Epoch 40  -- accuracy 0.002 - loss 0.057\n",
      "Epoch 41  -- accuracy 0.002 - loss 0.057\n",
      "Epoch 42  -- accuracy 0.002 - loss 0.056\n",
      "Epoch 43  -- accuracy 0.002 - loss 0.056\n",
      "Epoch 44  -- accuracy 0.002 - loss 0.055\n",
      "Epoch 45  -- accuracy 0.002 - loss 0.055\n",
      "Epoch 46  -- accuracy 0.002 - loss 0.054\n",
      "Epoch 47  -- accuracy 0.002 - loss 0.054\n",
      "Epoch 48  -- accuracy 0.002 - loss 0.053\n",
      "Epoch 49  -- accuracy 0.002 - loss 0.053\n",
      "Epoch 50  -- accuracy 0.002 - loss 0.052\n",
      "Epoch 51  -- accuracy 0.002 - loss 0.052\n",
      "Epoch 52  -- accuracy 0.002 - loss 0.051\n",
      "Epoch 53  -- accuracy 0.002 - loss 0.051\n",
      "Epoch 54  -- accuracy 0.002 - loss 0.051\n",
      "Epoch 55  -- accuracy 0.002 - loss 0.050\n",
      "Epoch 56  -- accuracy 0.002 - loss 0.050\n",
      "Epoch 57  -- accuracy 0.002 - loss 0.049\n",
      "Epoch 58  -- accuracy 0.002 - loss 0.049\n",
      "Epoch 59  -- accuracy 0.002 - loss 0.049\n",
      "Epoch 60  -- accuracy 0.002 - loss 0.048\n",
      "Epoch 61  -- accuracy 0.002 - loss 0.048\n",
      "Epoch 62  -- accuracy 0.003 - loss 0.048\n",
      "Epoch 63  -- accuracy 0.003 - loss 0.048\n",
      "Epoch 64  -- accuracy 0.004 - loss 0.047\n",
      "Epoch 65  -- accuracy 0.004 - loss 0.047\n",
      "Epoch 66  -- accuracy 0.005 - loss 0.047\n",
      "Epoch 67  -- accuracy 0.006 - loss 0.047\n",
      "Epoch 68  -- accuracy 0.006 - loss 0.046\n",
      "Epoch 69  -- accuracy 0.007 - loss 0.046\n",
      "Epoch 70  -- accuracy 0.008 - loss 0.046\n",
      "Epoch 71  -- accuracy 0.008 - loss 0.046\n",
      "Epoch 72  -- accuracy 0.009 - loss 0.046\n",
      "Epoch 73  -- accuracy 0.009 - loss 0.046\n",
      "Epoch 74  -- accuracy 0.010 - loss 0.045\n",
      "Epoch 75  -- accuracy 0.010 - loss 0.045\n",
      "Epoch 76  -- accuracy 0.011 - loss 0.045\n",
      "Epoch 77  -- accuracy 0.011 - loss 0.045\n",
      "Epoch 78  -- accuracy 0.012 - loss 0.045\n",
      "Epoch 79  -- accuracy 0.012 - loss 0.045\n",
      "Epoch 80  -- accuracy 0.012 - loss 0.045\n",
      "Epoch 81  -- accuracy 0.013 - loss 0.044\n",
      "Epoch 82  -- accuracy 0.013 - loss 0.044\n",
      "Epoch 83  -- accuracy 0.013 - loss 0.044\n",
      "Epoch 84  -- accuracy 0.013 - loss 0.044\n",
      "Epoch 85  -- accuracy 0.013 - loss 0.044\n",
      "Epoch 86  -- accuracy 0.014 - loss 0.044\n",
      "Epoch 87  -- accuracy 0.014 - loss 0.044\n",
      "Epoch 88  -- accuracy 0.014 - loss 0.044\n",
      "Epoch 89  -- accuracy 0.014 - loss 0.044\n",
      "Epoch 90  -- accuracy 0.014 - loss 0.044\n",
      "Epoch 91  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 92  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 93  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 94  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 95  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 96  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 97  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 98  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 99  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 100  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 101  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 102  -- accuracy 0.014 - loss 0.043\n",
      "Epoch 103  -- accuracy 0.014 - loss 0.042\n",
      "Epoch 104  -- accuracy 0.014 - loss 0.042\n",
      "Epoch 105  -- accuracy 0.015 - loss 0.042\n",
      "Epoch 106  -- accuracy 0.014 - loss 0.042\n",
      "Epoch 107  -- accuracy 0.014 - loss 0.042\n",
      "Epoch 108  -- accuracy 0.014 - loss 0.042\n",
      "Epoch 109  -- accuracy 0.014 - loss 0.042\n",
      "Epoch 110  -- accuracy 0.014 - loss 0.042\n",
      "Epoch 111  -- accuracy 0.014 - loss 0.042\n",
      "Epoch 112  -- accuracy 0.014 - loss 0.042\n",
      "Epoch 113  -- accuracy 0.015 - loss 0.042\n",
      "Epoch 114  -- accuracy 0.015 - loss 0.042\n",
      "Epoch 115  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 116  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 117  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 118  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 119  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 120  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 121  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 122  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 123  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 124  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 125  -- accuracy 0.015 - loss 0.041\n",
      "Epoch 126  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 127  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 128  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 129  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 130  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 131  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 132  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 133  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 134  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 135  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 136  -- accuracy 0.015 - loss 0.040\n",
      "Epoch 137  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 138  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 139  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 140  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 141  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 142  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 143  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 144  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 145  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 146  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 147  -- accuracy 0.015 - loss 0.039\n",
      "Epoch 148  -- accuracy 0.015 - loss 0.038\n",
      "Epoch 149  -- accuracy 0.015 - loss 0.038\n"
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "\n",
    "code_dim = code_dims[1]\n",
    "batch_size = batch_sizes[1]\n",
    "\n",
    "for learning_rate in learning_rates[::2]:\n",
    "    autoencoder = Autoencoder(\n",
    "        input_dim=flatten_dim, code_dim=code_dim, encoder_hidden_count=0, reduce_by=1.5\n",
    "    )\n",
    "    optimizer = Optimizer(\n",
    "        autoencoder,\n",
    "        loss=MeanSquaredError(),\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=150,\n",
    "    )\n",
    "\n",
    "    optimizer.fit(X_train, X_train)\n",
    "\n",
    "    pickle_results(code_dim, learning_rate, batch_size, optimizer, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  -- accuracy 0.001 - loss 0.087\n",
      "Epoch 1  -- accuracy 0.004 - loss 0.048\n",
      "Epoch 2  -- accuracy 0.005 - loss 0.039\n",
      "Epoch 3  -- accuracy 0.008 - loss 0.035\n",
      "Epoch 4  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 5  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 6  -- accuracy 0.012 - loss 0.029\n",
      "Epoch 7  -- accuracy 0.013 - loss 0.028\n",
      "Epoch 8  -- accuracy 0.013 - loss 0.027\n",
      "Epoch 9  -- accuracy 0.012 - loss 0.027\n",
      "Epoch 10  -- accuracy 0.012 - loss 0.027\n",
      "Epoch 11  -- accuracy 0.011 - loss 0.027\n",
      "Epoch 12  -- accuracy 0.011 - loss 0.026\n",
      "Epoch 13  -- accuracy 0.011 - loss 0.026\n",
      "Epoch 14  -- accuracy 0.011 - loss 0.026\n",
      "Epoch 15  -- accuracy 0.011 - loss 0.026\n",
      "Epoch 16  -- accuracy 0.011 - loss 0.026\n",
      "Epoch 17  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 18  -- accuracy 0.012 - loss 0.026\n",
      "Epoch 19  -- accuracy 0.012 - loss 0.025\n",
      "Epoch 20  -- accuracy 0.012 - loss 0.025\n",
      "Epoch 21  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 22  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 23  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 24  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 25  -- accuracy 0.011 - loss 0.025\n",
      "Epoch 26  -- accuracy 0.011 - loss 0.024\n",
      "Epoch 27  -- accuracy 0.011 - loss 0.024\n",
      "Epoch 28  -- accuracy 0.011 - loss 0.024\n",
      "Epoch 29  -- accuracy 0.011 - loss 0.024\n",
      "Epoch 30  -- accuracy 0.011 - loss 0.024\n",
      "Epoch 31  -- accuracy 0.011 - loss 0.024\n",
      "Epoch 32  -- accuracy 0.011 - loss 0.023\n",
      "Epoch 33  -- accuracy 0.011 - loss 0.023\n",
      "Epoch 34  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 35  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 36  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 37  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 38  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 39  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 40  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 41  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 42  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 43  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 44  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 45  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 46  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 47  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 48  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 49  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 50  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 51  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 52  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 53  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 54  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 55  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 56  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 57  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 58  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 59  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 60  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 61  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 62  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 63  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 64  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 65  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 66  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 67  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 68  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 69  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 70  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 71  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 72  -- accuracy 0.012 - loss 0.023\n",
      "Epoch 73  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 74  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 75  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 76  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 77  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 78  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 79  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 80  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 81  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 82  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 83  -- accuracy 0.011 - loss 0.022\n",
      "Epoch 84  -- accuracy 0.011 - loss 0.022\n",
      "Epoch 85  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 86  -- accuracy 0.012 - loss 0.022\n",
      "Epoch 87  -- accuracy 0.011 - loss 0.022\n",
      "Epoch 88  -- accuracy 0.011 - loss 0.022\n",
      "Epoch 89  -- accuracy 0.011 - loss 0.022\n",
      "Epoch 90  -- accuracy 0.011 - loss 0.022\n",
      "Epoch 91  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 92  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 93  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 94  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 95  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 96  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 97  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 98  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 99  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 100  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 101  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 102  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 103  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 104  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 105  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 106  -- accuracy 0.011 - loss 0.021\n",
      "Epoch 107  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 108  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 109  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 110  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 111  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 112  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 113  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 114  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 115  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 116  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 117  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 118  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 119  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 120  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 121  -- accuracy 0.012 - loss 0.021\n",
      "Epoch 122  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 123  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 124  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 125  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 126  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 127  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 128  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 129  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 130  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 131  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 132  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 133  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 134  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 135  -- accuracy 0.013 - loss 0.021\n",
      "Epoch 136  -- accuracy 0.013 - loss 0.020\n",
      "Epoch 137  -- accuracy 0.013 - loss 0.020\n",
      "Epoch 138  -- accuracy 0.013 - loss 0.020\n",
      "Epoch 139  -- accuracy 0.013 - loss 0.020\n",
      "Epoch 140  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 141  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 142  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 143  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 144  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 145  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 146  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 147  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 148  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 149  -- accuracy 0.014 - loss 0.020\n",
      "Epoch 0  -- accuracy 0.001 - loss 0.180\n",
      "Epoch 1  -- accuracy 0.002 - loss 0.141\n",
      "Epoch 2  -- accuracy 0.002 - loss 0.116\n",
      "Epoch 3  -- accuracy 0.002 - loss 0.098\n",
      "Epoch 4  -- accuracy 0.002 - loss 0.086\n",
      "Epoch 5  -- accuracy 0.002 - loss 0.079\n",
      "Epoch 6  -- accuracy 0.003 - loss 0.074\n",
      "Epoch 7  -- accuracy 0.003 - loss 0.071\n",
      "Epoch 8  -- accuracy 0.003 - loss 0.069\n",
      "Epoch 9  -- accuracy 0.003 - loss 0.067\n",
      "Epoch 10  -- accuracy 0.003 - loss 0.066\n",
      "Epoch 11  -- accuracy 0.003 - loss 0.065\n",
      "Epoch 12  -- accuracy 0.003 - loss 0.064\n",
      "Epoch 13  -- accuracy 0.003 - loss 0.063\n",
      "Epoch 14  -- accuracy 0.003 - loss 0.062\n",
      "Epoch 15  -- accuracy 0.003 - loss 0.061\n",
      "Epoch 16  -- accuracy 0.003 - loss 0.059\n",
      "Epoch 17  -- accuracy 0.003 - loss 0.058\n",
      "Epoch 18  -- accuracy 0.003 - loss 0.057\n",
      "Epoch 19  -- accuracy 0.002 - loss 0.056\n",
      "Epoch 20  -- accuracy 0.002 - loss 0.055\n",
      "Epoch 21  -- accuracy 0.002 - loss 0.054\n",
      "Epoch 22  -- accuracy 0.002 - loss 0.053\n",
      "Epoch 23  -- accuracy 0.002 - loss 0.052\n",
      "Epoch 24  -- accuracy 0.002 - loss 0.051\n",
      "Epoch 25  -- accuracy 0.002 - loss 0.050\n",
      "Epoch 26  -- accuracy 0.002 - loss 0.049\n",
      "Epoch 27  -- accuracy 0.002 - loss 0.049\n",
      "Epoch 28  -- accuracy 0.003 - loss 0.048\n",
      "Epoch 29  -- accuracy 0.003 - loss 0.048\n",
      "Epoch 30  -- accuracy 0.004 - loss 0.047\n",
      "Epoch 31  -- accuracy 0.005 - loss 0.047\n",
      "Epoch 32  -- accuracy 0.006 - loss 0.047\n",
      "Epoch 33  -- accuracy 0.008 - loss 0.046\n",
      "Epoch 34  -- accuracy 0.009 - loss 0.046\n",
      "Epoch 35  -- accuracy 0.009 - loss 0.046\n",
      "Epoch 36  -- accuracy 0.010 - loss 0.046\n",
      "Epoch 37  -- accuracy 0.011 - loss 0.046\n",
      "Epoch 38  -- accuracy 0.011 - loss 0.045\n",
      "Epoch 39  -- accuracy 0.010 - loss 0.045\n",
      "Epoch 40  -- accuracy 0.010 - loss 0.045\n",
      "Epoch 41  -- accuracy 0.010 - loss 0.045\n",
      "Epoch 42  -- accuracy 0.010 - loss 0.045\n",
      "Epoch 43  -- accuracy 0.009 - loss 0.045\n",
      "Epoch 44  -- accuracy 0.009 - loss 0.045\n",
      "Epoch 45  -- accuracy 0.009 - loss 0.044\n",
      "Epoch 46  -- accuracy 0.009 - loss 0.044\n",
      "Epoch 47  -- accuracy 0.008 - loss 0.044\n",
      "Epoch 48  -- accuracy 0.008 - loss 0.044\n",
      "Epoch 49  -- accuracy 0.008 - loss 0.044\n",
      "Epoch 50  -- accuracy 0.008 - loss 0.043\n",
      "Epoch 51  -- accuracy 0.007 - loss 0.043\n",
      "Epoch 52  -- accuracy 0.007 - loss 0.043\n",
      "Epoch 53  -- accuracy 0.007 - loss 0.043\n",
      "Epoch 54  -- accuracy 0.007 - loss 0.043\n",
      "Epoch 55  -- accuracy 0.007 - loss 0.043\n",
      "Epoch 56  -- accuracy 0.007 - loss 0.042\n",
      "Epoch 57  -- accuracy 0.007 - loss 0.042\n",
      "Epoch 58  -- accuracy 0.007 - loss 0.042\n",
      "Epoch 59  -- accuracy 0.006 - loss 0.042\n",
      "Epoch 60  -- accuracy 0.006 - loss 0.042\n",
      "Epoch 61  -- accuracy 0.006 - loss 0.041\n",
      "Epoch 62  -- accuracy 0.007 - loss 0.041\n",
      "Epoch 63  -- accuracy 0.006 - loss 0.041\n",
      "Epoch 64  -- accuracy 0.006 - loss 0.041\n",
      "Epoch 65  -- accuracy 0.007 - loss 0.041\n",
      "Epoch 66  -- accuracy 0.007 - loss 0.040\n",
      "Epoch 67  -- accuracy 0.006 - loss 0.040\n",
      "Epoch 68  -- accuracy 0.006 - loss 0.040\n",
      "Epoch 69  -- accuracy 0.006 - loss 0.040\n",
      "Epoch 70  -- accuracy 0.007 - loss 0.040\n",
      "Epoch 71  -- accuracy 0.006 - loss 0.039\n",
      "Epoch 72  -- accuracy 0.007 - loss 0.039\n",
      "Epoch 73  -- accuracy 0.007 - loss 0.039\n",
      "Epoch 74  -- accuracy 0.007 - loss 0.039\n",
      "Epoch 75  -- accuracy 0.007 - loss 0.039\n",
      "Epoch 76  -- accuracy 0.007 - loss 0.038\n",
      "Epoch 77  -- accuracy 0.007 - loss 0.038\n",
      "Epoch 78  -- accuracy 0.007 - loss 0.038\n",
      "Epoch 79  -- accuracy 0.007 - loss 0.038\n",
      "Epoch 80  -- accuracy 0.008 - loss 0.038\n",
      "Epoch 81  -- accuracy 0.007 - loss 0.037\n",
      "Epoch 82  -- accuracy 0.008 - loss 0.037\n",
      "Epoch 83  -- accuracy 0.008 - loss 0.037\n",
      "Epoch 84  -- accuracy 0.007 - loss 0.037\n",
      "Epoch 85  -- accuracy 0.008 - loss 0.037\n",
      "Epoch 86  -- accuracy 0.007 - loss 0.037\n",
      "Epoch 87  -- accuracy 0.008 - loss 0.036\n",
      "Epoch 88  -- accuracy 0.008 - loss 0.036\n",
      "Epoch 89  -- accuracy 0.008 - loss 0.036\n",
      "Epoch 90  -- accuracy 0.008 - loss 0.036\n",
      "Epoch 91  -- accuracy 0.008 - loss 0.036\n",
      "Epoch 92  -- accuracy 0.008 - loss 0.035\n",
      "Epoch 93  -- accuracy 0.008 - loss 0.035\n",
      "Epoch 94  -- accuracy 0.008 - loss 0.035\n",
      "Epoch 95  -- accuracy 0.008 - loss 0.035\n",
      "Epoch 96  -- accuracy 0.009 - loss 0.035\n",
      "Epoch 97  -- accuracy 0.009 - loss 0.035\n",
      "Epoch 98  -- accuracy 0.009 - loss 0.035\n",
      "Epoch 99  -- accuracy 0.009 - loss 0.034\n",
      "Epoch 100  -- accuracy 0.009 - loss 0.034\n",
      "Epoch 101  -- accuracy 0.009 - loss 0.034\n",
      "Epoch 102  -- accuracy 0.009 - loss 0.034\n",
      "Epoch 103  -- accuracy 0.009 - loss 0.034\n",
      "Epoch 104  -- accuracy 0.009 - loss 0.034\n",
      "Epoch 105  -- accuracy 0.009 - loss 0.034\n",
      "Epoch 106  -- accuracy 0.009 - loss 0.034\n",
      "Epoch 107  -- accuracy 0.009 - loss 0.033\n",
      "Epoch 108  -- accuracy 0.009 - loss 0.033\n",
      "Epoch 109  -- accuracy 0.009 - loss 0.033\n",
      "Epoch 110  -- accuracy 0.009 - loss 0.033\n",
      "Epoch 111  -- accuracy 0.010 - loss 0.033\n",
      "Epoch 112  -- accuracy 0.009 - loss 0.033\n",
      "Epoch 113  -- accuracy 0.009 - loss 0.033\n",
      "Epoch 114  -- accuracy 0.010 - loss 0.033\n",
      "Epoch 115  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 116  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 117  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 118  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 119  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 120  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 121  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 122  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 123  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 124  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 125  -- accuracy 0.010 - loss 0.032\n",
      "Epoch 126  -- accuracy 0.010 - loss 0.031\n",
      "Epoch 127  -- accuracy 0.010 - loss 0.031\n",
      "Epoch 128  -- accuracy 0.010 - loss 0.031\n",
      "Epoch 129  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 130  -- accuracy 0.010 - loss 0.031\n",
      "Epoch 131  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 132  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 133  -- accuracy 0.010 - loss 0.031\n",
      "Epoch 134  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 135  -- accuracy 0.010 - loss 0.031\n",
      "Epoch 136  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 137  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 138  -- accuracy 0.010 - loss 0.031\n",
      "Epoch 139  -- accuracy 0.011 - loss 0.031\n",
      "Epoch 140  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 141  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 142  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 143  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 144  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 145  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 146  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 147  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 148  -- accuracy 0.011 - loss 0.030\n",
      "Epoch 149  -- accuracy 0.011 - loss 0.030\n"
     ]
    }
   ],
   "source": [
    "# Batch size\n",
    "\n",
    "code_dim = code_dims[1]\n",
    "learning_rate = learning_rates[1]\n",
    "\n",
    "for batch_size in batch_sizes[::2]:\n",
    "    autoencoder = Autoencoder(\n",
    "        input_dim=flatten_dim, code_dim=code_dim, encoder_hidden_count=0, reduce_by=1.5\n",
    "    )\n",
    "    optimizer = Optimizer(\n",
    "        autoencoder,\n",
    "        loss=MeanSquaredError(),\n",
    "        accuracy=Accuracy(),\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=150,\n",
    "    )\n",
    "\n",
    "    optimizer.fit(X_train, X_train)\n",
    "\n",
    "    pickle_results(code_dim, learning_rate, batch_size, optimizer, autoencoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Injecting noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 22:31:26.468998: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-12 22:31:26.736882: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-12 22:31:26.739034: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-12 22:31:29.199938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 784))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset.dataset import load_fashion_mnist_dataset\n",
    "\n",
    "noisy_X_train, X_train = load_fashion_mnist_dataset(noisy=True, noise_level=0.08)\n",
    "\n",
    "noisy_X_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from numpy import random\n",
    "\n",
    "random_idx = random.randint(0, noisy_X_train.shape[0])\n",
    "\n",
    "plt.imshow(noisy_X_train[random_idx].reshape(28, 28), interpolation=\"nearest\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(X_train[random_idx].reshape(28, 28), interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  --  loss 0.067\n",
      "Epoch 1  --  loss 0.045\n",
      "Epoch 2  --  loss 0.039\n",
      "Epoch 3  --  loss 0.034\n",
      "Epoch 4  --  loss 0.030\n",
      "Epoch 5  --  loss 0.028\n",
      "Epoch 6  --  loss 0.027\n",
      "Epoch 7  --  loss 0.026\n",
      "Epoch 8  --  loss 0.025\n",
      "Epoch 9  --  loss 0.025\n",
      "Epoch 10  --  loss 0.024\n",
      "Epoch 11  --  loss 0.023\n",
      "Epoch 12  --  loss 0.023\n",
      "Epoch 13  --  loss 0.022\n",
      "Epoch 14  --  loss 0.022\n",
      "Epoch 15  --  loss 0.022\n",
      "Epoch 16  --  loss 0.021\n",
      "Epoch 17  --  loss 0.021\n",
      "Epoch 18  --  loss 0.020\n",
      "Epoch 19  --  loss 0.020\n",
      "Epoch 20  --  loss 0.020\n",
      "Epoch 21  --  loss 0.019\n",
      "Epoch 22  --  loss 0.019\n",
      "Epoch 23  --  loss 0.019\n",
      "Epoch 24  --  loss 0.019\n",
      "Epoch 25  --  loss 0.018\n",
      "Epoch 26  --  loss 0.018\n",
      "Epoch 27  --  loss 0.018\n",
      "Epoch 28  --  loss 0.018\n",
      "Epoch 29  --  loss 0.017\n",
      "Epoch 30  --  loss 0.017\n",
      "Epoch 31  --  loss 0.017\n",
      "Epoch 32  --  loss 0.017\n",
      "Epoch 33  --  loss 0.017\n",
      "Epoch 34  --  loss 0.016\n",
      "Epoch 35  --  loss 0.016\n",
      "Epoch 36  --  loss 0.016\n",
      "Epoch 37  --  loss 0.016\n",
      "Epoch 38  --  loss 0.016\n",
      "Epoch 39  --  loss 0.016\n",
      "Epoch 40  --  loss 0.015\n",
      "Epoch 41  --  loss 0.015\n",
      "Epoch 42  --  loss 0.015\n",
      "Epoch 43  --  loss 0.015\n",
      "Epoch 44  --  loss 0.015\n",
      "Epoch 45  --  loss 0.015\n",
      "Epoch 46  --  loss 0.015\n",
      "Epoch 47  --  loss 0.015\n",
      "Epoch 48  --  loss 0.014\n",
      "Epoch 49  --  loss 0.014\n",
      "Epoch 50  --  loss 0.014\n",
      "Epoch 51  --  loss 0.014\n",
      "Epoch 52  --  loss 0.014\n",
      "Epoch 53  --  loss 0.014\n",
      "Epoch 54  --  loss 0.014\n",
      "Epoch 55  --  loss 0.014\n",
      "Epoch 56  --  loss 0.014\n",
      "Epoch 57  --  loss 0.014\n",
      "Epoch 58  --  loss 0.014\n",
      "Epoch 59  --  loss 0.013\n",
      "Epoch 60  --  loss 0.013\n",
      "Epoch 61  --  loss 0.013\n",
      "Epoch 62  --  loss 0.013\n",
      "Epoch 63  --  loss 0.013\n",
      "Epoch 64  --  loss 0.013\n",
      "Epoch 65  --  loss 0.013\n",
      "Epoch 66  --  loss 0.013\n",
      "Epoch 67  --  loss 0.013\n",
      "Epoch 68  --  loss 0.013\n",
      "Epoch 69  --  loss 0.013\n",
      "Epoch 70  --  loss 0.013\n",
      "Epoch 71  --  loss 0.013\n",
      "Epoch 72  --  loss 0.013\n",
      "Epoch 73  --  loss 0.013\n",
      "Epoch 74  --  loss 0.012\n",
      "Epoch 75  --  loss 0.012\n",
      "Epoch 76  --  loss 0.012\n",
      "Epoch 77  --  loss 0.012\n",
      "Epoch 78  --  loss 0.012\n",
      "Epoch 79  --  loss 0.012\n",
      "Epoch 80  --  loss 0.012\n",
      "Epoch 81  --  loss 0.012\n",
      "Epoch 82  --  loss 0.012\n",
      "Epoch 83  --  loss 0.012\n",
      "Epoch 84  --  loss 0.012\n",
      "Epoch 85  --  loss 0.012\n",
      "Epoch 86  --  loss 0.012\n",
      "Epoch 87  --  loss 0.012\n",
      "Epoch 88  --  loss 0.012\n",
      "Epoch 89  --  loss 0.012\n",
      "Epoch 90  --  loss 0.012\n",
      "Epoch 91  --  loss 0.012\n",
      "Epoch 92  --  loss 0.012\n",
      "Epoch 93  --  loss 0.012\n",
      "Epoch 94  --  loss 0.012\n",
      "Epoch 95  --  loss 0.011\n",
      "Epoch 96  --  loss 0.011\n",
      "Epoch 97  --  loss 0.011\n",
      "Epoch 98  --  loss 0.011\n",
      "Epoch 99  --  loss 0.011\n",
      "Epoch 100  --  loss 0.011\n",
      "Epoch 101  --  loss 0.011\n",
      "Epoch 102  --  loss 0.011\n",
      "Epoch 103  --  loss 0.011\n",
      "Epoch 104  --  loss 0.011\n",
      "Epoch 105  --  loss 0.011\n",
      "Epoch 106  --  loss 0.011\n",
      "Epoch 107  --  loss 0.011\n",
      "Epoch 108  --  loss 0.011\n",
      "Epoch 109  --  loss 0.011\n",
      "Epoch 110  --  loss 0.011\n",
      "Epoch 111  --  loss 0.011\n",
      "Epoch 112  --  loss 0.011\n",
      "Epoch 113  --  loss 0.011\n",
      "Epoch 114  --  loss 0.011\n",
      "Epoch 115  --  loss 0.011\n",
      "Epoch 116  --  loss 0.011\n",
      "Epoch 117  --  loss 0.011\n",
      "Epoch 118  --  loss 0.011\n",
      "Epoch 119  --  loss 0.011\n",
      "Epoch 120  --  loss 0.011\n",
      "Epoch 121  --  loss 0.011\n",
      "Epoch 122  --  loss 0.011\n",
      "Epoch 123  --  loss 0.011\n",
      "Epoch 124  --  loss 0.011\n",
      "Epoch 125  --  loss 0.011\n",
      "Epoch 126  --  loss 0.011\n",
      "Epoch 127  --  loss 0.010\n",
      "Epoch 128  --  loss 0.010\n",
      "Epoch 129  --  loss 0.010\n",
      "Epoch 130  --  loss 0.010\n",
      "Epoch 131  --  loss 0.010\n",
      "Epoch 132  --  loss 0.010\n",
      "Epoch 133  --  loss 0.010\n",
      "Epoch 134  --  loss 0.010\n",
      "Epoch 135  --  loss 0.010\n",
      "Epoch 136  --  loss 0.010\n",
      "Epoch 137  --  loss 0.010\n",
      "Epoch 138  --  loss 0.010\n",
      "Epoch 139  --  loss 0.010\n",
      "Epoch 140  --  loss 0.010\n",
      "Epoch 141  --  loss 0.010\n",
      "Epoch 142  --  loss 0.010\n",
      "Epoch 143  --  loss 0.010\n",
      "Epoch 144  --  loss 0.010\n",
      "Epoch 145  --  loss 0.010\n",
      "Epoch 146  --  loss 0.010\n",
      "Epoch 147  --  loss 0.010\n",
      "Epoch 148  --  loss 0.010\n",
      "Epoch 149  --  loss 0.010\n",
      "Epoch 150  --  loss 0.010\n",
      "Epoch 151  --  loss 0.010\n",
      "Epoch 152  --  loss 0.010\n",
      "Epoch 153  --  loss 0.010\n",
      "Epoch 154  --  loss 0.010\n",
      "Epoch 155  --  loss 0.010\n",
      "Epoch 156  --  loss 0.010\n",
      "Epoch 157  --  loss 0.010\n",
      "Epoch 158  --  loss 0.010\n",
      "Epoch 159  --  loss 0.010\n",
      "Epoch 160  --  loss 0.010\n",
      "Epoch 161  --  loss 0.010\n",
      "Epoch 162  --  loss 0.010\n",
      "Epoch 163  --  loss 0.010\n",
      "Epoch 164  --  loss 0.010\n",
      "Epoch 165  --  loss 0.010\n",
      "Epoch 166  --  loss 0.010\n",
      "Epoch 167  --  loss 0.010\n",
      "Epoch 168  --  loss 0.010\n",
      "Epoch 169  --  loss 0.010\n",
      "Epoch 170  --  loss 0.010\n",
      "Epoch 171  --  loss 0.010\n",
      "Epoch 172  --  loss 0.010\n",
      "Epoch 173  --  loss 0.010\n",
      "Epoch 174  --  loss 0.010\n",
      "Epoch 175  --  loss 0.010\n",
      "Epoch 176  --  loss 0.010\n",
      "Epoch 177  --  loss 0.010\n",
      "Epoch 178  --  loss 0.010\n",
      "Epoch 179  --  loss 0.010\n",
      "Epoch 180  --  loss 0.010\n",
      "Epoch 181  --  loss 0.009\n",
      "Epoch 182  --  loss 0.009\n",
      "Epoch 183  --  loss 0.009\n",
      "Epoch 184  --  loss 0.009\n",
      "Epoch 185  --  loss 0.009\n",
      "Epoch 186  --  loss 0.009\n",
      "Epoch 187  --  loss 0.009\n",
      "Epoch 188  --  loss 0.009\n",
      "Epoch 189  --  loss 0.009\n",
      "Epoch 190  --  loss 0.009\n",
      "Epoch 191  --  loss 0.009\n",
      "Epoch 192  --  loss 0.009\n",
      "Epoch 193  --  loss 0.009\n",
      "Epoch 194  --  loss 0.009\n",
      "Epoch 195  --  loss 0.009\n",
      "Epoch 196  --  loss 0.009\n",
      "Epoch 197  --  loss 0.009\n",
      "Epoch 198  --  loss 0.009\n",
      "Epoch 199  --  loss 0.009\n",
      "Epoch 200  --  loss 0.009\n",
      "Epoch 201  --  loss 0.009\n",
      "Epoch 202  --  loss 0.009\n",
      "Epoch 203  --  loss 0.009\n",
      "Epoch 204  --  loss 0.009\n",
      "Epoch 205  --  loss 0.009\n",
      "Epoch 206  --  loss 0.009\n",
      "Epoch 207  --  loss 0.009\n",
      "Epoch 208  --  loss 0.009\n",
      "Epoch 209  --  loss 0.009\n",
      "Epoch 210  --  loss 0.009\n",
      "Epoch 211  --  loss 0.009\n",
      "Epoch 212  --  loss 0.009\n",
      "Epoch 213  --  loss 0.009\n",
      "Epoch 214  --  loss 0.009\n",
      "Epoch 215  --  loss 0.009\n",
      "Epoch 216  --  loss 0.009\n",
      "Epoch 217  --  loss 0.009\n",
      "Epoch 218  --  loss 0.009\n",
      "Epoch 219  --  loss 0.009\n",
      "Epoch 220  --  loss 0.009\n",
      "Epoch 221  --  loss 0.009\n",
      "Epoch 222  --  loss 0.009\n",
      "Epoch 223  --  loss 0.009\n",
      "Epoch 224  --  loss 0.009\n",
      "Epoch 225  --  loss 0.009\n",
      "Epoch 226  --  loss 0.009\n",
      "Epoch 227  --  loss 0.009\n",
      "Epoch 228  --  loss 0.009\n",
      "Epoch 229  --  loss 0.009\n",
      "Epoch 230  --  loss 0.009\n",
      "Epoch 231  --  loss 0.009\n",
      "Epoch 232  --  loss 0.009\n",
      "Epoch 233  --  loss 0.009\n",
      "Epoch 234  --  loss 0.009\n",
      "Epoch 235  --  loss 0.009\n",
      "Epoch 236  --  loss 0.009\n",
      "Epoch 237  --  loss 0.009\n",
      "Epoch 238  --  loss 0.009\n",
      "Epoch 239  --  loss 0.009\n",
      "Epoch 240  --  loss 0.009\n",
      "Epoch 241  --  loss 0.009\n",
      "Epoch 242  --  loss 0.009\n",
      "Epoch 243  --  loss 0.009\n",
      "Epoch 244  --  loss 0.009\n",
      "Epoch 245  --  loss 0.009\n",
      "Epoch 246  --  loss 0.009\n",
      "Epoch 247  --  loss 0.009\n",
      "Epoch 248  --  loss 0.009\n",
      "Epoch 249  --  loss 0.009\n",
      "Epoch 250  --  loss 0.009\n",
      "Epoch 251  --  loss 0.009\n",
      "Epoch 252  --  loss 0.009\n",
      "Epoch 253  --  loss 0.009\n",
      "Epoch 254  --  loss 0.009\n",
      "Epoch 255  --  loss 0.009\n",
      "Epoch 256  --  loss 0.009\n",
      "Epoch 257  --  loss 0.009\n",
      "Epoch 258  --  loss 0.009\n",
      "Epoch 259  --  loss 0.009\n",
      "Epoch 260  --  loss 0.009\n",
      "Epoch 261  --  loss 0.009\n",
      "Epoch 262  --  loss 0.009\n",
      "Epoch 263  --  loss 0.009\n",
      "Epoch 264  --  loss 0.009\n",
      "Epoch 265  --  loss 0.009\n",
      "Epoch 266  --  loss 0.009\n",
      "Epoch 267  --  loss 0.009\n",
      "Epoch 268  --  loss 0.009\n",
      "Epoch 269  --  loss 0.009\n",
      "Epoch 270  --  loss 0.009\n",
      "Epoch 271  --  loss 0.009\n",
      "Epoch 272  --  loss 0.009\n",
      "Epoch 273  --  loss 0.009\n",
      "Epoch 274  --  loss 0.009\n",
      "Epoch 275  --  loss 0.009\n",
      "Epoch 276  --  loss 0.009\n",
      "Epoch 277  --  loss 0.009\n",
      "Epoch 278  --  loss 0.009\n",
      "Epoch 279  --  loss 0.009\n",
      "Epoch 280  --  loss 0.009\n",
      "Epoch 281  --  loss 0.009\n",
      "Epoch 282  --  loss 0.009\n",
      "Epoch 283  --  loss 0.009\n",
      "Epoch 284  --  loss 0.009\n",
      "Epoch 285  --  loss 0.009\n",
      "Epoch 286  --  loss 0.009\n",
      "Epoch 287  --  loss 0.009\n",
      "Epoch 288  --  loss 0.009\n",
      "Epoch 289  --  loss 0.009\n",
      "Epoch 290  --  loss 0.009\n",
      "Epoch 291  --  loss 0.009\n",
      "Epoch 292  --  loss 0.009\n",
      "Epoch 293  --  loss 0.009\n",
      "Epoch 294  --  loss 0.009\n",
      "Epoch 295  --  loss 0.009\n",
      "Epoch 296  --  loss 0.009\n",
      "Epoch 297  --  loss 0.009\n",
      "Epoch 298  --  loss 0.009\n",
      "Epoch 299  --  loss 0.009\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from models.metrics import MeanSquaredError\n",
    "from models.neural_network import Autoencoder\n",
    "from models.optimizer import Optimizer\n",
    "\n",
    "autoencoder = Autoencoder(\n",
    "    input_dim=flatten_dim, code_dim=121, encoder_hidden_count=1, reduce_by=1.5\n",
    ")\n",
    "optimizer = Optimizer(\n",
    "    autoencoder,\n",
    "    loss=MeanSquaredError(),\n",
    "    learning_rate=0.9,\n",
    "    batch_size=128,\n",
    "    epochs=300,\n",
    ")\n",
    "\n",
    "optimizer.fit(noisy_X_train, X_train)\n",
    "\n",
    "path = \"saved_models/\"\n",
    "noise_path = \"noise/\"\n",
    "\n",
    "with open(f\"{path}networks/{noise_path}autoencoder\", \"wb\") as pickle_file:\n",
    "    pickle.dump(autoencoder, pickle_file)\n",
    "\n",
    "with open(f\"{path}optimizers/{noise_path}optimizer\", \"wb\") as pickle_file:\n",
    "    pickle.dump(optimizer, pickle_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"saved_models/\"\n",
    "early_stopping_path = \"early_stopping/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from models.metrics import MeanSquaredError\n",
    "from models.neural_network import Autoencoder\n",
    "from models.optimizer import Optimizer\n",
    "\n",
    "from dataset.dataset import load_fashion_mnist_dataset\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = load_fashion_mnist_dataset(y=True)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "flatten_dim = X_train[0].shape[0]\n",
    "\n",
    "autoencoder = Autoencoder(\n",
    "    input_dim=flatten_dim, code_dim=121, encoder_hidden_count=1, reduce_by=1.5\n",
    ")\n",
    "optimizer = Optimizer(\n",
    "    autoencoder,\n",
    "    loss=MeanSquaredError(),\n",
    "    learning_rate=0.9,\n",
    "    batch_size=128,\n",
    "    epochs=500,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "optimizer.fit(X_train, X_train, X_valid, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"saved_models/\"\n",
    "early_stopping_path = \"early_stopping/\"\n",
    "\n",
    "with open(f\"{path}networks/{early_stopping_path}autoencoder\", \"wb\") as pickle_file:\n",
    "    pickle.dump(autoencoder, pickle_file)\n",
    "\n",
    "with open(f\"{path}optimizers/{early_stopping_path}optimizer\", \"wb\") as pickle_file:\n",
    "    pickle.dump(optimizer, pickle_file)\n",
    "\n",
    "\n",
    "optimizer.validation_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classification model on raw fashion mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset.dataset import load_fashion_mnist_dataset\n",
    "\n",
    "X_train, y_train, _, _ = load_fashion_mnist_dataset(y=True)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  -- accuracy 0.160 - loss 2.249\n",
      "Epoch 1  -- accuracy 0.106 - loss 2.304\n",
      "Epoch 2  -- accuracy 0.102 - loss 2.304\n",
      "Epoch 3  -- accuracy 0.111 - loss 2.292\n",
      "Epoch 4  -- accuracy 0.104 - loss 2.304\n",
      "Epoch 5  -- accuracy 0.102 - loss 2.304\n",
      "Epoch 6  -- accuracy 0.102 - loss 2.304\n",
      "Epoch 7  -- accuracy 0.102 - loss 2.304\n"
     ]
    }
   ],
   "source": [
    "from models.neural_network import ClassificationNeuralNetwork\n",
    "from models.optimizer import Optimizer\n",
    "from models.metrics import CategoricalCrossEntropyLoss, Accuracy\n",
    "\n",
    "\n",
    "flatten_dim = X_train[0].shape[0]\n",
    "\n",
    "network = ClassificationNeuralNetwork(\n",
    "    input_dim=flatten_dim,\n",
    "    hidden_dim=350,\n",
    "    output_dim=10,\n",
    "    number_of_hidden_layers=1,\n",
    ")\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    network,\n",
    "    accuracy=Accuracy(),\n",
    "    loss=CategoricalCrossEntropyLoss(),\n",
    "    learning_rate=0.85,\n",
    "    batch_size=128,\n",
    "    epochs=8,\n",
    "    reshape=False,\n",
    ")\n",
    "\n",
    "optimizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = \"saved_models/\"\n",
    "classification_path = \"fashion_classification/\"\n",
    "\n",
    "with open(f\"{path}networks/{classification_path}model\", \"wb\") as pickle_file:\n",
    "    pickle.dump(network, pickle_file)\n",
    "\n",
    "with open(f\"{path}optimizers/{classification_path}optimizer\", \"wb\") as pickle_file:\n",
    "    pickle.dump(optimizer, pickle_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classification model on encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray\n",
    "import pickle\n",
    "\n",
    "from models.neural_network import Autoencoder\n",
    "\n",
    "\n",
    "path = \"saved_models/\"\n",
    "\n",
    "\n",
    "def get_encoded_dataset(network: Autoencoder, image: NDArray):\n",
    "    return network.get_encoded(image, len(network.layers) // 2 + 1)\n",
    "\n",
    "\n",
    "with open(f\"{path}networks/autoencoder_36_0.9_16\", \"rb\") as pickle_file:\n",
    "    fashion_autoencoder = pickle.load(pickle_file)\n",
    "\n",
    "X_train_encoded = get_encoded_dataset(fashion_autoencoder, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 36)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  -- accuracy 0.494 - loss 1.326\n",
      "Epoch 1  -- accuracy 0.730 - loss 0.684\n",
      "Epoch 2  -- accuracy 0.767 - loss 0.593\n",
      "Epoch 3  -- accuracy 0.786 - loss 0.551\n",
      "Epoch 4  -- accuracy 0.799 - loss 0.524\n",
      "Epoch 5  -- accuracy 0.810 - loss 0.502\n",
      "Epoch 6  -- accuracy 0.816 - loss 0.485\n",
      "Epoch 7  -- accuracy 0.822 - loss 0.471\n"
     ]
    }
   ],
   "source": [
    "network = ClassificationNeuralNetwork(\n",
    "    input_dim=X_train_encoded.shape[1],\n",
    "    hidden_dim=350,\n",
    "    output_dim=10,\n",
    "    number_of_hidden_layers=1,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    network,\n",
    "    accuracy=Accuracy(),\n",
    "    loss=CategoricalCrossEntropyLoss(),\n",
    "    learning_rate=0.85,\n",
    "    batch_size=128,\n",
    "    epochs=8,\n",
    "    reshape=False,\n",
    ")\n",
    "\n",
    "optimizer.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path}networks/{classification_path}model_on_encoded\", \"wb\") as pickle_file:\n",
    "    pickle.dump(network, pickle_file)\n",
    "\n",
    "with open(\n",
    "    f\"{path}optimizers/{classification_path}optimizer_on_encoded\", \"wb\"\n",
    ") as pickle_file:\n",
    "    pickle.dump(optimizer, pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (main, Jan 15 2022, 18:02:07) [GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
